{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","private_outputs":true,"authorship_tag":"ABX9TyPv4rHzwTo+uSCMROjodqLZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["SetUp and Data Preprocessing"],"metadata":{"id":"hvx-KH0qVDXF"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"1aimwjpmUAIX"},"outputs":[],"source":["import os\n","import random\n","import tarfile\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset, ConcatDataset, Subset\n","import torchvision.transforms as transforms\n","from PIL import Image\n","from timm import create_model\n","\n","# --- Utility to Extract TAR Files Safely ---\n","def extract_tar(file_path, extract_to):\n","    with tarfile.open(file_path, 'r') as tar:\n","        def is_within_directory(directory, target):\n","            abs_directory = os.path.abspath(directory)\n","            abs_target = os.path.abspath(target)\n","            return abs_target.startswith(abs_directory)\n","\n","        def safe_extract(tar, path=\".\", members=None):\n","            for member in tar.getmembers():\n","                member_path = os.path.join(path, member.name)\n","                if not is_within_directory(path, member_path):\n","                    raise Exception(\"Attempted Path Traversal in Tar File\")\n","            tar.extractall(path, members=members)\n","\n","        safe_extract(tar, path=extract_to)\n","\n","# --- Validate Directories ---\n","def validate_directory(path, name):\n","    if not os.path.isdir(path):\n","        raise FileNotFoundError(f\"{name} directory not found: {path}\")\n","\n","# --- Custom Dataset ---\n","class CustomDataset(Dataset):\n","    def __init__(self, data_dir, label, transform=None):\n","        self.data_dir = data_dir\n","        self.label = label\n","        self.transform = transform\n","        self.image_paths = [os.path.join(data_dir, img) for img in os.listdir(data_dir)]\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.image_paths[idx]\n","        img = Image.open(img_path).convert(\"RGB\")\n","        if self.transform:\n","            img = self.transform(img)\n","        return img, self.label\n","\n","# --- Data Preparation ---\n","train_real_dir = r'D:\\SPS\\train_real\\real'\n","train_fake_dir = r'D:\\SPS\\train_fake\\fake'\n","valid_real_dir = r'D:\\SPS\\valid_real\\real'\n","valid_fake_dir = r'D:\\SPS\\valid_fake\\fake'\n","\n","# Validate directories\n","validate_directory(train_real_dir, \"Train Real\")\n","validate_directory(train_fake_dir, \"Train Fake\")\n","validate_directory(valid_real_dir, \"Valid Real\")\n","validate_directory(valid_fake_dir, \"Valid Fake\")\n","\n","# Define transformations\n","train_transforms = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","\n","val_transforms = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","\n","# Load datasets\n","train_real_dataset = CustomDataset(train_real_dir, label=0, transform=train_transforms)\n","train_fake_dataset = CustomDataset(train_fake_dir, label=1, transform=train_transforms)\n","val_real_dataset = CustomDataset(valid_real_dir, label=0, transform=val_transforms)\n","val_fake_dataset = CustomDataset(valid_fake_dir, label=1, transform=val_transforms)\n","\n","# Combine datasets\n","train_dataset = ConcatDataset([train_real_dataset, train_fake_dataset])\n","val_dataset = ConcatDataset([val_real_dataset, val_fake_dataset])\n","\n","# Create DataLoaders\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n","\n","# --- Model Definition ---\n","model = create_model('vit_base_patch16_224', pretrained=True, num_classes=1)\n","model.head = nn.Sequential(\n","    nn.Linear(model.head.in_features, 1),\n","    nn.Sigmoid()\n",")\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = model.to(device)\n","\n","# Define loss and optimizer\n","criterion = nn.BCELoss()\n","optimizer = optim.Adam(model.parameters(), lr=1e-4)\n","\n","# --- Training and Validation Functions ---\n","def train_model(model, train_loader, val_loader, epochs=10, optimizer=optimizer, criterion=criterion):\n","    model.train()\n","    for epoch in range(epochs):\n","        running_loss = 0.0\n","        correct_predictions = 0\n","        total_predictions = 0\n","\n","        for inputs, labels in train_loader:\n","            inputs, labels = inputs.to(device), labels.to(device).float()\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs.view(-1), labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","            predictions = (outputs > 0.5).float()\n","            correct_predictions += (predictions == labels).sum().item()\n","            total_predictions += labels.size(0)\n","\n","        epoch_loss = running_loss / len(train_loader)\n","        epoch_accuracy = (correct_predictions / total_predictions) * 100\n","        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n","        validate_model(model, val_loader)\n","\n","def validate_model(model, val_loader):\n","    model.eval()\n","    correct_predictions = 0\n","    total_predictions = 0\n","\n","    with torch.no_grad():\n","        for inputs, labels in val_loader:\n","            inputs, labels = inputs.to(device), labels.to(device).float()\n","            outputs = model(inputs)\n","            predictions = (outputs > 0.5).float()\n","            correct_predictions += (predictions == labels).sum().item()\n","            total_predictions += labels.size(0)\n","\n","    val_accuracy = (correct_predictions / total_predictions) * 100\n","    print(f\"Validation Accuracy: {val_accuracy:.2f}%\")\n","\n","# --- Predict Function ---\n","def predict_image(model, img_path):\n","    model.eval()\n","    img = Image.open(img_path).convert(\"RGB\")\n","    img = val_transforms(img).unsqueeze(0).to(device)\n","    with torch.no_grad():\n","        output = model(img)\n","        prediction = (output > 0.5).float().item()\n","    return \"Fake\" if prediction == 1 else \"Real\"\n","\n","# --- Train and Test ---\n","train_model(model, train_loader, val_loader, epochs=10)\n","\n","# Test with a sample image\n","img_path = r'D:\\SPS\\valid_real\\real\\valid_real_0029961.png'\n","result = predict_image(model, img_path)\n","print(f\"The image is classified as: {result}\")\n"]},{"cell_type":"code","source":["!pip install Pillow\n"],"metadata":{"id":"9ZiFdLjipb63"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Data Augmentation and Transformation"],"metadata":{"id":"AuPygqP_UeGH"}},{"cell_type":"code","source":["import os\n","import torch\n","from torch.utils.data import DataLoader, Dataset\n","import torchvision.transforms as transforms\n","from PIL import Image\n","\n","# Define dataset paths\n","train_real_dir = r'D:\\SPS\\train_real\\real'\n","train_fake_dir = r'D:\\SPS\\train_fake\\fake'\n","valid_real_dir = r'D:\\SPS\\valid_real\\real'\n","valid_fake_dir = r'D:\\SPS\\valid_fake\\fake'\n","\n","# Validate directories\n","def validate_directory(path, name):\n","    if not os.path.isdir(path):\n","        raise FileNotFoundError(f\"{name} directory not found: {path}\")\n","\n","validate_directory(train_real_dir, \"Train Real\")\n","validate_directory(train_fake_dir, \"Train Fake\")\n","validate_directory(valid_real_dir, \"Valid Real\")\n","validate_directory(valid_fake_dir, \"Valid Fake\")\n","\n","# Custom Dataset Class\n","class CustomDataset(Dataset):\n","    def __init__(self, data_dir, label, transform=None):\n","        self.data_dir = data_dir\n","        self.label = label\n","        self.transform = transform\n","        self.image_paths = [os.path.join(data_dir, img) for img in os.listdir(data_dir)]\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.image_paths[idx]\n","        img = Image.open(img_path).convert(\"RGB\")  # Ensure images are RGB\n","        if self.transform:\n","            img = self.transform(img)\n","        return img, self.label\n","\n","# Define Data Transformations\n","train_transforms = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","\n","val_transforms = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","\n","# Load training data\n","train_real_dataset = CustomDataset(train_real_dir, label=0, transform=train_transforms)\n","train_fake_dataset = CustomDataset(train_fake_dir, label=1, transform=train_transforms)\n","\n","# Load validation data\n","val_real_dataset = CustomDataset(valid_real_dir, label=0, transform=val_transforms)\n","val_fake_dataset = CustomDataset(valid_fake_dir, label=1, transform=val_transforms)\n","\n","# Combine datasets\n","train_dataset = torch.utils.data.ConcatDataset([train_real_dataset, train_fake_dataset])\n","val_dataset = torch.utils.data.ConcatDataset([val_real_dataset, val_fake_dataset])\n","\n","# Data Loaders\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n","\n","# Verify data loading\n","for images, labels in train_loader:\n","    print(f\"Batch images shape: {images.shape}\")\n","    print(f\"Batch labels: {labels}\")\n","    break\n"],"metadata":{"id":"it2-69OdUZEB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"EaRK86qq4oyd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"AwFveFmnhNkq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["python -m pip3 install --upgrade pip\n","\n","\n"],"metadata":{"id":"Wa5gimEehM7w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install timm\n"],"metadata":{"id":"NtZiCwot_lIE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Load Pre-Trained ViT Model"],"metadata":{"id":"_2yk0PWBU61F"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from timm import create_model\n","\n","# Model Creation\n","model = create_model('vit_base_patch16_224', pretrained=True, num_classes=1)\n","\n","# Modify the model head for binary classification (sigmoid output)\n","model.head = nn.Sequential(\n","    nn.Linear(model.head.in_features, 1),  # One output unit\n","    nn.Sigmoid()  # Sigmoid activation for binary classification\n",")\n","\n","# Device configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = model.to(device)\n","\n","# Loss function (binary cross-entropy)\n","criterion = nn.BCELoss()\n","\n","# Optimizer (Adam optimizer)\n","optimizer = optim.Adam(model.parameters(), lr=1e-4)\n","\n","# Verify model, loss function, and optimizer\n","print(model)\n","print(criterion)\n","print(optimizer)\n"],"metadata":{"id":"_Mrx3Q9DVKLF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from timm import create_model\n","from torch.utils.data import DataLoader, Subset\n","import random\n","\n","# Define the model with binary classification output\n","model = create_model('vit_base_patch16_224', pretrained=True, num_classes=1)\n","model.head = nn.Sequential(\n","    nn.Linear(model.head.in_features, 1),\n","    nn.Sigmoid()\n",")\n","\n","# Define the device, criterion, and optimizer\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = model.to(device)\n","\n","criterion = nn.BCELoss()\n","optimizer = optim.Adam(model.parameters(), lr=1e-4)\n","\n","# Randomly select a subset from the dataset\n","def get_random_subset(dataset, subset_size=300):\n","    total_samples = len(dataset)\n","    subset_indices = random.sample(range(total_samples), subset_size)\n","    return Subset(dataset, subset_indices)\n","\n","# Training function\n","def train_model(model, train_loader, val_loader, epochs=10, optimizer=optimizer, criterion=criterion):\n","    model.train()\n","\n","    for epoch in range(epochs):\n","        running_loss = 0.0\n","        correct_predictions = 0\n","        total_predictions = 0\n","\n","        # Training phase\n","        for batch_idx, (inputs, labels) in enumerate(train_loader):\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            optimizer.zero_grad()\n","\n","            # Forward pass\n","            outputs = model(inputs)\n","            loss = criterion(outputs.view(-1), labels.float())\n","\n","            # Backward pass and optimization\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","            predictions = (outputs > 0.5).float()\n","            correct_predictions += (predictions == labels).sum().item()\n","            total_predictions += labels.size(0)\n","\n","        # Epoch statistics\n","        epoch_loss = running_loss / len(train_loader)\n","        epoch_accuracy = (correct_predictions / total_predictions) * 100\n","        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n","\n","        # Validation phase\n","        validate_model(model, val_loader)\n","\n","# Validation function\n","def validate_model(model, val_loader):\n","    model.eval()\n","    correct_predictions = 0\n","    total_predictions = 0\n","\n","    with torch.no_grad():\n","        for inputs, labels in val_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = model(inputs)\n","           # Inside the training loop\n","            predictions = (outputs.view(-1) > 0.5).float()  # Reshape outputs to 1D and threshold\n","            correct_predictions += (predictions == labels.float()).sum().item()  # Ensure labels are float\n","            total_predictions += labels.size(0)  # Total number of samples\n","\n","\n","    val_accuracy = (correct_predictions / total_predictions) * 100\n","    print(f\"Validation Accuracy: {val_accuracy:.2f}%\")\n","\n","# Prepare DataLoader for a subset\n","subset_size = 300\n","train_subset = get_random_subset(train_dataset, subset_size=subset_size)  # Ensure train_dataset is defined\n","train_subset_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n","\n","# Train the model using the subset DataLoader\n","train_model(model, train_subset_loader, val_loader, epochs=10)\n"],"metadata":{"id":"S6HEmB0QWkjG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Training the Model"],"metadata":{"id":"dAsPkk02Vmgz"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from timm import create_model\n","\n","# Define the model with binary classification output\n","model = create_model('vit_base_patch16_224', pretrained=True, num_classes=1)\n","model.head = nn.Sequential(\n","    nn.Linear(model.head.in_features, 1),\n","    nn.Sigmoid()\n",")\n","\n","# Define the device, criterion, and optimizer\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = model.to(device)\n","\n","criterion = nn.BCELoss()\n","optimizer = optim.Adam(model.parameters(), lr=1e-4)\n","\n","# Training function\n","def train_model(model, train_loader, val_loader, epochs=10, optimizer=optimizer, criterion=criterion):\n","    model.train()\n","\n","    for epoch in range(epochs):\n","        running_loss = 0.0\n","        correct_predictions = 0\n","        total_predictions = 0\n","\n","        # Training phase\n","        for batch_idx, (inputs, labels) in enumerate(train_loader):\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            optimizer.zero_grad()\n","\n","            # Forward pass\n","            outputs = model(inputs)\n","            loss = criterion(outputs.view(-1), labels.float())  # Ensure output matches the label shape\n","\n","            # Backward pass and optimization\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","            predictions = (outputs > 0.5).float()  # Convert outputs to binary predictions\n","            correct_predictions += (predictions == labels).sum().item()\n","            total_predictions += labels.size(0)\n","\n","            # Print the processed images count every 100 batches\n","            if (batch_idx + 1) % 100 == 0:\n","                print(f\"Epoch [{epoch+1}/{epochs}], Processed {total_predictions}/{len(train_loader.dataset)} images\")\n","\n","        # Calculate epoch loss and accuracy\n","        epoch_loss = running_loss / len(train_loader)\n","        epoch_accuracy = (correct_predictions / total_predictions) * 100\n","        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n","\n","        # Validation phase\n","        validate_model(model, val_loader)\n","\n","# Validation function\n","def validate_model(model, val_loader):\n","    model.eval()\n","    correct_predictions = 0\n","    total_predictions = 0\n","\n","    with torch.no_grad():\n","        for inputs, labels in val_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = model(inputs)\n","            predictions = (outputs > 0.5).float()  # Convert outputs to binary predictions\n","            correct_predictions += (predictions == labels).sum().item()\n","            total_predictions += labels.size(0)\n","\n","    val_accuracy = (correct_predictions / total_predictions) * 100\n","    print(f\"Validation Accuracy: {val_accuracy:.2f}%\")\n","\n","# Training the model\n","train_model(model, train_loader, val_loader, epochs=10)\n"],"metadata":{"id":"W7MM7GkHVln9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Evaluate the model"],"metadata":{"id":"ico74ZbOVr5I"}},{"cell_type":"code","source":["def predict_image(model, img_path):\n","    model.eval()\n","    img = Image.open(img_path).convert(\"RGB\")\n","    img = val_transforms(img).unsqueeze(0).to(device)\n","\n","    with torch.no_grad():\n","        output = model(img)\n","        prediction = (output > 0.5).float().item()\n","\n","    return \"Fake\" if prediction == 1 else \"Real\"\n","\n","\n","img_path = r'D:\\SPS\\valid_real\\real\\valid_real_0029961.png'\n","result = predict_image(model, img_path)\n","print(f\"The image is classified as: {result}\")\n"],"metadata":{"id":"mG38TH60Vsec"},"execution_count":null,"outputs":[]}]}